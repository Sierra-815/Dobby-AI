# -*- coding: utf-8 -*-
"""kobert_korquad.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OL1Hg0kGSOCNJmhfRrpuhJndO7wvvGZr

1/20 소연 수정: 학습시킨 모델 저장해서 불러오는 방식으로 수정
* 매번 git clone 하지 말고 그냥 통째 zip파일로 shareddrive에 넣어놔도 되긴 하는데 clone 자체는 별로 오래 안걸리므로 지현언니의 선택으로 놔두겠습니다~!
"""

from google.colab import drive
drive.mount("/content/drive", force_remount=True)
!git clone https://github.com/monologg/KoBERT-KorQuAD
!pip install transformers==2.9.1
import os, sys
# permanently install packages
# nb_path = '/content/notebooks'
# os.symlink('/content/drive/Shareddrives/KPMG_Ideation', nb_path)
# sys.path.insert(0, nb_path)
# !pip install --target=$nb_path sentencepiece

import json
f = open ('/content/KoBERT-KorQuAD/data/KorQuAD_v1.0_dev.json', "r") 
# Reading from file 
data = json.loads(f.read()) 
# Closing file 
f.close()

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/KoBERT-KorQuAD
from transformers import BertModel
from tokenization_kobert import KoBertTokenizer
import argparse
import glob
import logging
import os
import random
import timeit

import numpy as np
import torch
from torch.utils.data import DataLoader, RandomSampler, SequentialSampler
from torch.utils.data.distributed import DistributedSampler
from tqdm import tqdm, trange

from evaluate_v1_0 import eval_during_train

from transformers import (
    WEIGHTS_NAME,
    AdamW,
    AlbertConfig,
    AlbertForQuestionAnswering,
    AlbertTokenizer,
    BertConfig,
    BertForQuestionAnswering,
    BertTokenizer,
    DistilBertConfig,
    DistilBertForQuestionAnswering,
    DistilBertTokenizer,
    RobertaConfig,
    RobertaForQuestionAnswering,
    RobertaTokenizer,
    XLMConfig,
    XLMForQuestionAnswering,
    XLMTokenizer,
    XLNetConfig,
    XLNetForQuestionAnswering,
    XLNetTokenizer,
    get_linear_schedule_with_warmup,
    squad_convert_examples_to_features
)
from transformers.data.metrics.squad_metrics import (
    compute_predictions_log_probs,
    compute_predictions_logits,
    squad_evaluate)

#@title Fine-tuning with korquad 1.0 (Do NOT Re-run)
# no need to rerun
# !python3 run_squad.py --model_type distilkobert \
#                        --model_name_or_path monologg/distilkobert \
#                        --output_dir ../drive/MyDrive/kobert_korquad/models \
#                        --data_dir data \
#                        --train_file KorQuAD_v1.0_train.json \
#                        --predict_file KorQuAD_v1.0_dev.json \
#                        --evaluate_during_training \
#                        --per_gpu_train_batch_size 8 \
#                        --per_gpu_eval_batch_size 8 \
#                        --max_seq_length 512 \
#                        --logging_steps 4000 \
#                        --save_steps 4000 \
#                        --do_train

config_class = DistilBertConfig
tokenizer_class = KoBertTokenizer
model_class = DistilBertForQuestionAnswering
model = model_class.from_pretrained('/content/drive/Shareddrives/KPMG_Ideation/models')  # , force_download=True)
tokenizer = tokenizer_class.from_pretrained('/content/drive/Shareddrives/KPMG_Ideation/models', do_lower_case=True)

# text = '그리고 갤럭시 S5에 탑재되었다가 갤럭시 S6/S6 엣지에서 도로 삭제되었던 방수 방진을 \
# 다시 지원한다. 등급은 IP68로, 이는 방진 등급으로나 방수 등급으로나 모두 최고레벨이며 \
# 갤럭시 S5보다도 높은 등급이다. 또한, 심장 박동 인식 센서가 전작인 갤럭시 S6와 동일하게 후면 카메라 모듈 옆에 존재한다. \
# 다만, 안드로이드 6.0 마시멜로가 AOSP 단에서 기본적으로 지원하기 시작한 USB Type-C가 아닌 micro 5핀이라 불리는 기존 규격인 USB micro Type-B를 입출력 단자로 사용한다. \
# 이는 USB Type-C로 아직 캡리스 방수 솔루션이 준비되지 않았고 여기에 삼성 기어 VR과의 호환성 문제도 존재하기 때문이다.',

text=  '2016년 9월 4일 중동 지역에서 갤럭시 S7 엣지가 폭발하는 사건이 발생했다. 이는 갤럭시 노트 7 폭발 사건과 관련이 있다고 추측되고 있으나, 삼성전자 측에서는 갤럭시 S7, 갤럭시 S7 엣지는 안전상의 문제가 없다고 해명하였으나, 폭발 사고가 갤럭시 S7 엣지 출시 이후 6개월 정도가 지난 후이며 판매량 또한 7000만대가량이 넘게 팔린 후에 처음 일어난 것을 생각하면 폭발한 기기들이 불량인 제품이고 모든 제품의 결함이라고 보기는 어렵다. 실제로 모든 제품이 결함이 있는 갤럭시 노트 7은 겨우 출시 18일만이며 판매량이 200만대 정도일때 수십대가 터졌다.하드웨어 문제인지는 몰라도 자기혼자 후레쉬가 켜져있는 경우도 있다. 또한 밝기를 최대밝기로 하고 자동밝기 조절을 하지 않았으나 자동밝기와 같은 현상이 나타난다'

questions = [
   '폭발 사고는 출시 이후 몇개월 정도가 지났을때 발생했나?',
   '폭발 사고는 언제 발생했나?',
   '어떤 문제가 있었나?'
]

for question in questions:
    inputs = tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors="pt")
    del inputs["token_type_ids"]

    input_ids = inputs["input_ids"].tolist()[0]

    text_tokens = tokenizer.convert_ids_to_tokens(input_ids)
    answer_start_scores, answer_end_scores = model(**inputs)

    answer_start = torch.argmax(
        answer_start_scores
    )  # Get the most likely beginning of answer with the argmax of the score
    answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score

    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))

    print(f"Question: {question}")
    print(f"Answer: {answer}\n")

text=  ' 세부적인 내용은 이제 개발자분들 중심적으로 해 주시면 될 거 같고 \
    그다음에 이제 저희가 비슷한 모델 백신 같은 경우에도 비슷한 모델 같은 것도 많이 찾아 주시길 \
     했는데 저희가 시간 관계 사고로 더 많이 그래도 아직은 찾지 못해 좀 더 논문 같은 거나 \
     이런 사례가 더 많이 있는 직접적으로 존나 찾아보기로 하고요. 일단 셋으로 나누는 게 좋을 거 \
      같아요. 그래서 저희가 1번이 제안서를 한번 빌려 타이핑으로라도 한번 각자 한번 써 보는 걸로 \
      각자 분야별로 그걸 이제 나중에 저희가 합치면 되는 거니까 그렇게 했으면 좋겠어요.'
questions = [
   '세부 내용은 누가 담당하는가?'
]

for question in questions:
    inputs = tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors="pt")
    del inputs["token_type_ids"]

    input_ids = inputs["input_ids"].tolist()[0]

    text_tokens = tokenizer.convert_ids_to_tokens(input_ids)
    answer_start_scores, answer_end_scores = model(**inputs)

    answer_start = torch.argmax(
        answer_start_scores
    )  # Get the most likely beginning of answer with the argmax of the score
    answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score

    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))

    print(f"Question: {question}")
    print(f"Answer: {answer}\n")



"""추가 학습 데이터셋: 
* 코쿼드 2.0 (얜 위키 한 페이지 전체로 학습시키는 거라 문서 길이가 녹음 대본만큼 길다는 장점이 있지만 표, 그래프 등도 읽도록 학습되기 때문에 사실상 크게 필요 있을 지는 모르겠음)
* 한국어 Babi 데이터셋 (짧은 문장 위주 but tagged)
https://bit.ly/31SqtHy
* TyDi-QA (한국어)
https://github.com/google-research-datasets/tydiqa


"""