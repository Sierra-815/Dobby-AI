{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "STT_with_full_text.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVv82PyAvSHR"
      },
      "source": [
        "1/14 최초 구현 by 소연 \r\n",
        "\r\n",
        "수정 및 테스트 시 본 파일이 아닌 사본 사용을 부탁드립니다.\r\n",
        "\r\n",
        "\r\n",
        "1/18  수정 by 지현\r\n",
        "\r\n",
        "* 회의록 음성 파일 일정 크기로 자르는 loop추가 --> 회의 전체 내용이 chked에 들어가도록 하였음\r\n",
        "* 음성 파일 중 일부(2개) .UnknownValueError 발생하여 제거 --> 제거한 채로 갈지, 따로 처리해줘야 할지?\r\n",
        "\r\n",
        "1/18 수정 by 소연\r\n",
        "\r\n",
        "* UnknownValueError 발생하는 이슈 해결\r\n",
        "* STT loop 함수화 --> 나중에 다른 것도 잘되는거 확인하면 spellchecker랑 class화 합시당"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bd82roy9fTZg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e429fc93-7155-44a3-f5a4-dd5e9a7fc19b"
      },
      "source": [
        "import os, sys\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "%cd /content/drive/Shareddrives/KPMG_Ideation\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings('ignore')\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from pprint import pprint\r\n",
        "from krwordrank.word import KRWordRank\r\n",
        "from copy import deepcopy\r\n",
        "import kss\r\n",
        "import itertools\r\n",
        "import unicodedata\r\n",
        "import requests\r\n",
        "from functools import reduce\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "import string\r\n",
        "import torch\r\n",
        "from textrankr import TextRank\r\n",
        "from lexrankr import LexRank\r\n",
        "from nltk.corpus import stopwords \r\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize \r\n",
        "from pydub import AudioSegment\r\n",
        "from konlpy.tag import Okt\r\n",
        "import re\r\n",
        "import nltk\r\n",
        "# nltk.download('punkt')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/Shareddrives/KPMG_Ideation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxdbYb9RonDm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0427274d-3bd7-413f-f9ab-b3c033373939"
      },
      "source": [
        "# import pre-trained model -- frameBERT (pytorch GPU 환경 필요)\r\n",
        "%cd /content/drive/Shareddrives/KPMG_Ideation/OpenInformationExtraction/frameBERT\r\n",
        "!pip install transformers \r\n",
        "import frame_parser\r\n",
        "path=\"/content/drive/Shareddrives/KPMG_Ideation/OpenInformationExtraction/frameBERT\"\r\n",
        "parser = frame_parser.FrameParser(model_path=path, language='ko')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/Shareddrives/KPMG_Ideation/OpenInformationExtraction/frameBERT\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.2.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "srl model: framenet\n",
            "language: ko\n",
            "version: 1.2\n",
            "using viterbi: False\n",
            "using masking: True\n",
            "pretrained BERT: bert-base-multilingual-cased\n",
            "using TGT special token: True\n",
            "used dictionary:\n",
            "\t /content/drive/Shareddrives/KPMG_Ideation/OpenInformationExtraction/frameBERT/src/../koreanframenet/resource/info/kfn1.2_lu2idx.json\n",
            "\t /content/drive/Shareddrives/KPMG_Ideation/OpenInformationExtraction/frameBERT/src/../koreanframenet/resource/info/kfn1.2_lufrmap.json\n",
            "\t /content/drive/Shareddrives/KPMG_Ideation/OpenInformationExtraction/frameBERT/src/../koreanframenet/resource/info/mul_bio_frargmap.json\n",
            "...loaded model path: /content/drive/Shareddrives/KPMG_Ideation/OpenInformationExtraction/frameBERT\n",
            "/content/drive/Shareddrives/KPMG_Ideation/OpenInformationExtraction/frameBERT\n",
            "...model is loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8w5FQbH1ffsM"
      },
      "source": [
        "##### below are permanently installed packages #####\r\n",
        "# nb_path = '/content/notebooks'\r\n",
        "# os.symlink('/content/drive/Shareddrives/KPMG_Ideation', nb_path)\r\n",
        "# sys.path.insert(0, nb_path)\r\n",
        "# !pip install --target=$nb_path pydub\r\n",
        "# !pip install --target=$nb_path kss\r\n",
        "# %cd /content/drive/Shareddrives/KPMG_Ideation/hanspell\r\n",
        "# !python setup.py install\r\n",
        "# !pip install --target=$nb_path transformers\r\n",
        "# !apt-get update\r\n",
        "# !apt-get g++ openjdk-8-jdk \r\n",
        "# !pip3 install --target=$nb_path konlpy\r\n",
        "# !pip install --target=$nb_path soykeyword\r\n",
        "# !pip install --target=$nb_path krwordrank\r\n",
        "# !pip install --target=$nb_path bert\r\n",
        "# !pip install --target=$nb_path textrankr\r\n",
        "# !pip install --target=$nb_path lexrankr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYdHPgYzIaen",
        "outputId": "95e22262-1e8c-4b7f-cf4b-4b50617fbf7f"
      },
      "source": [
        "# Due to google api credentials, SpeechRecognition needs to be installed everytime\r\n",
        "!pip install SpeechRecognition\r\n",
        "import speech_recognition as sr\r\n",
        "#!pip install google-api-core==1.22.2\r\n",
        "# !pip install --upgrade google-api-core\r\n",
        "# !pip install --upgrade google-cloud-speech"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: SpeechRecognition in /usr/local/lib/python3.6/dist-packages (3.8.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZ1pXksLKwVX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRYLQ4tzgZF6"
      },
      "source": [
        "files_path = ''\r\n",
        "file_name = ''\r\n",
        "def STT_loop(file_name, startMin = 0, startSec = 0, endMin = 4, endSec = 30):\r\n",
        "    if file_name.split('.')[1] == 'mp3':    \r\n",
        "        sound = AudioSegment.from_mp3(file_name)\r\n",
        "        wav_filename = file_name.split('.')[0] + '.wav'\r\n",
        "        file_handle = sound.export(file_name, format=\"wav\")\r\n",
        "        song = AudioSegment.from_wav(wav_filename)\r\n",
        "    if file_name.split('.')[1] == 'm4a':\r\n",
        "        track = AudioSegment.from_file(file_name,'m4a')\r\n",
        "        wav_filename = file_name.replace('m4a', 'wav')\r\n",
        "        file_handle = track.export(wav_filename, format='wav') \r\n",
        "        song = AudioSegment.from_wav(wav_filename)\r\n",
        "    \r\n",
        "    # Time to miliseconds\r\n",
        "    startTime = startMin*60*1000+startSec*1000\r\n",
        "    endTime = endMin*60*1000+endSec*1000\r\n",
        "\r\n",
        "    num_frac=int(len(song)/endTime)\r\n",
        "    for i in range(num_frac+1):\r\n",
        "      extract = song[startTime:min(endTime,len(song))]\r\n",
        "      extract.export('./audio_frac/frac_'+str(i)+'.wav', format=\"wav\")\r\n",
        "      startTime=endTime\r\n",
        "      endTime+=endMin*60*1000+endSec*1000\r\n",
        "\r\n",
        "    # Speech Recognition Loop\r\n",
        "    txt=''\r\n",
        "    for i in range(num_frac+1):\r\n",
        "      AUDIO_FILE = os.path.join(os.path.dirname(os.path.abspath('data')), \"audio_frac/frac_\"+str(i)+\".wav\")\r\n",
        "\r\n",
        "      # use the audio file as the audio source\r\n",
        "      r = sr.Recognizer()\r\n",
        "      with sr.AudioFile(AUDIO_FILE) as source:\r\n",
        "          audio = r.record(source)  # read the entire audio file\r\n",
        "\r\n",
        "      # recognize speech using Google Speech Recognition\r\n",
        "      try:\r\n",
        "          # for testing purposes, we're just using the default API key\r\n",
        "          # to use another API key, use `r.recognize_google(audio, key=\"GOOGLE_SPEECH_RECOGNITION_API_KEY\")`\r\n",
        "          # instead of `r.recognize_google(audio)`\r\n",
        "          txt += ' '+ r.recognize_google(audio, language='ko')\r\n",
        "          print(str(i)+\"/\"+str(num_frac+1)+\" done\")\r\n",
        "          #print(\"Google Speech Recognition:\" + txt)\r\n",
        "      except sr.UnknownValueError:\r\n",
        "          print(\"Google Speech Recognition could not understand audio\")\r\n",
        "      except sr.RequestError as e:\r\n",
        "          print(\"Could not request results from Google Speech Recognition service; {0}\".format(e))\r\n",
        "      \r\n",
        "    return txt"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZkwcXFPJVYa",
        "outputId": "77b6c650-01a1-4ffc-82e7-cbfcb8625a04"
      },
      "source": [
        "%cd /content/drive/Shareddrives/KPMG_Ideation/data\r\n",
        "file_name='audio_only_1.m4a'\r\n",
        "txt = STT_loop(file_name)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/Shareddrives/KPMG_Ideation/data\n",
            "0/18 done\n",
            "1/18 done\n",
            "2/18 done\n",
            "3/18 done\n",
            "4/18 done\n",
            "5/18 done\n",
            "6/18 done\n",
            "7/18 done\n",
            "8/18 done\n",
            "9/18 done\n",
            "10/18 done\n",
            "11/18 done\n",
            "12/18 done\n",
            "13/18 done\n",
            "14/18 done\n",
            "15/18 done\n",
            "16/18 done\n",
            "17/18 done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSp9kYqTgaf-",
        "outputId": "923ec674-82aa-4a40-f8fb-abe70381fd4b"
      },
      "source": [
        "%cd /content/drive/Shareddrives/KPMG_Ideation/hanspell\r\n",
        "from hanspell import spell_checker\r\n",
        "chked=\"\"\r\n",
        "line = kss.split_sentences(txt)\r\n",
        "for i in range(len(line)):\r\n",
        "  if i%20==0:\r\n",
        "    print(str(i)+\"/\"+str(len(line))+\" lines done\")\r\n",
        "  line[i] = spell_checker.check(line[i])[2]\r\n",
        "  #print(\"Checked spelling \",line[i])\r\n",
        "  chked += \"\".join(line[i])\r\n",
        "  chked += \". \""
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/Shareddrives/KPMG_Ideation/hanspell\n",
            "0/290 lines done\n",
            "20/290 lines done\n",
            "40/290 lines done\n",
            "60/290 lines done\n",
            "80/290 lines done\n",
            "100/290 lines done\n",
            "120/290 lines done\n",
            "140/290 lines done\n",
            "160/290 lines done\n",
            "180/290 lines done\n",
            "200/290 lines done\n",
            "220/290 lines done\n",
            "240/290 lines done\n",
            "260/290 lines done\n",
            "280/290 lines done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "56-B2OSOc7L0",
        "outputId": "fe79c65f-bc6f-49a2-a79e-de12a6c380d9"
      },
      "source": [
        "chked[:1000]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'문제가 있었다. 그 알고리즘으로 막상 해 놨길래 했는데 거기서 그 의료인들이 거의 다 빠지고 1순위에서 의료인들이 제일 필요한데 개를 다 빠지고 다른데 접종을 하는 것 때문에 막 난리가 났다고 그래가지고 알고리즘을 다시 만나기로 했어 나 뭐 이런 얘기 하다. 그래서 거기서 어떻게 그 어떤 자료들을 사용을 한다면은 아까 말씀했듯이 그 바이러스 노출도 직업에 따른 바이러스 노출도 그러나 이 그걸 가진 크게 두 개로 받고 멜론에서 나이가 65세 이상 해결하는 거 아닌가 25세 이하라던가 하면은 그 조금 더 가산점을 주는 거라던가 아니면은 뭐 직업 특성상 뭐 그 의도가 이런데 종사하면 가산점을 주는 거고 근데 반면에 이제 조금 문제가 어떻게 될 수 있냐면 직업 특성 만 가지고 하면 내 막상 재택근무자 아들이라도 산업 군이 막 그런 그쪽에서 켜면은 태권도는 아무런 위험에 노출되어 있지 않은데 그런 식으로 바다 쪽으로 받아가지고 오히려 더 그런 그런 문제들이 아직 해결할 때 많이 남았다고 찾아왔어요. 굉장히 좋은 지적이십니다. 그렇다면 직업 특성상 재택근무가 아예 불가능한 집중된 섞여 있을 거 같은데 직업 특성뿐만이 아니라 그냥 바이러스 노출도 제일 중요한 거 같기도 하고 그 그게 이제 목적에 따라서 누굴 먼저 맞춰야 되니까 조금 다른게 그 감염률을 나 죽고 싶으면 네 사실 뭐랄까 제일 활발하게 활동하는 새끼들을 먼저 맞춰야지 그래야지 지금 뭐 그 퍼트리고 다니는 애들이 안 걸려 있으니까 그래서 그런 경우 조금 야외활동이라던가 이런 것들이 활동이 많은 젊은 층들이 맞춰야 되는 반면에 치사율을 낮추고자를 목표로 하면은 우선은 노인 인구가 가장 큰 높으니까 애들 먼저 맞춰야 된다 이런 일이 있어가지고 달달 말씀이시죠 목적에 따라 달라진다. 목적에 따라 준다. 그리고 또 다른 이슈가 될 만한 게 또 배탈 개 중에 하나가 박신 백신을 맞고 나서 이제 어느 정도로 확진자 수가 누적돼서 증가하는 야 그걸 또 어느 도착할 것으로 보는 경우도 많잖아요. 그러려면 그렇죠 그런데 그럴 때'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "k8EYw1Mwl8Us",
        "outputId": "1a73707e-3b84-4603-e571-34d5e3a9a7c0"
      },
      "source": [
        "chked[-1000:]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'연령 지역 그리고 아까 그 무슨 과목 얘기도 잠깐 나가서 어떤 거 같은데 그것처럼 약간 그런 변수 정말 넣어야 하는 변수들 이런 거 같아요. 그러면 각각 프로토타입을 작성을 하고 나중에 같이 이렇게 검술을 해서 제출하는 걸로 하는 거죠. 이게 친정을 아니거든 뭐 하고 저희가 나중에 다 같이 이거 결국은 짬뽕하면 되는 거고 하니까 저의 진짜 제안서는 같은 경우에도 저희가 사실 많이 섭취를 해 보겠음 이렇게 정리를 해 보자 정말 제안서를 제출한다는 식으로 저희가 정리를 갔다. 하는 시간을 가졌으면 합니다. 그게 또 의심이 있을 거 같아서 저희 생각 정리하는데 그러면은 그 언제 그 팀들이 알아서 그 뭐냐 그 이거를 완성에 와라 그 데이터를 정하면 될 거 같은데 아 그리고 까먹었는데 그리고도 돼 아까 데이터 얘기도 많이 나왔잖아요. 그래서 내가 이 모델을 했을 때 우리가 수집해야 할 데이터 구체적인 거 이런 데이터가 필요할 것 같다. 그리고 아까 그 백신 같은 경우에는 사실 저희가 정보를 의학지식이 정말 전문 지식이 있어야 되잖아요. 그런 지식을 우리가 뭘 어떻게 알아야 될 거 같다. 어떤 논문에 계속 참고해야 될 거 같고 어떤 쪽으로 우리가 지식을 키워야 될 거 같다. 이런 것도 포함해 주시면 괜찮을 거 그리고 버리게 6분 안에 승현 님 가시기 전에 날짜를 바꿔 보면은 표를 했을 때 저번에 아 금요일 토요일이 정보가 다 되고 그러면 7시에는 괜찮으신 건가요. 조금씩 뭐 좀 나는 좀 더 앞으로 당겨서 괜찮다 뭐 이런 건 있으세요. 저녁 시간이 애매해서 그냥 7시에는 괜찮으시면 7시 좋아요. 그러면 화요일은 통 정말 되는 사람이 별로 없고 수요일 금요일에 뭔가 너무 먼 기분이 나서 지금 제가 좀 꺼려지네요. 지금 수요일 같은 경우 안 된다고 하신 분이 가영이가 형님 아예 안 된대요 7시 수요일에 생일입니다. 목요일 14일네 알겠습니다. 괜찮아요. 그냥 목요일 알겠습니다. 그럼 목요일 7시까지 거리가 어떻게 돼 한서를 자기 방식으로 한번 정리를 해서 오는 걸로 합시다. '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Nzudf8Rm4b0",
        "outputId": "d32a7868-75dc-4635-cb68-9272c81139e3"
      },
      "source": [
        "np.array(['w'])\r\n",
        "%cd /content/drive/Shareddrives/KPMG_Ideation/data\r\n",
        "t=open('./texts/'+file_name.split('.')[0] + '.txt', \"w\")\r\n",
        "n=t.write(chked)\r\n",
        "t.close()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/Shareddrives/KPMG_Ideation/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjoeqHrKZDI_"
      },
      "source": [
        "okt = Okt()\r\n",
        "class Text(): \r\n",
        "    def __init__(self, text):\r\n",
        "        text = re.sub(\"'\", ' ', text)\r\n",
        "        paragraphs = text.split('\\n')\r\n",
        "        self.text = text\r\n",
        "        self.paragraphs = [i for i in paragraphs if i]\r\n",
        "        self.counts = len(self.paragraphs)\r\n",
        "        self.docs = [kss.split_sentences(paragraph) for paragraph in paragraphs if kss.split_sentences(paragraph)]\r\n",
        "        self.newtext = deepcopy(self.text)\r\n",
        "        print(\"TEXT\")\r\n",
        "\r\n",
        "    def findall(self, p, s):\r\n",
        "        i = s.find(p)\r\n",
        "        while i != -1:\r\n",
        "            yield i\r\n",
        "            i = s.find(p, i + 1)\r\n",
        "      \r\n",
        "    def countMatcher(self, sentences, paragraph_no):\r\n",
        "        paragraph = self.docs[paragraph_no]\r\n",
        "        total_no = len(paragraph)\r\n",
        "        vec = [0] * total_no\r\n",
        "        \r\n",
        "        for idx, candidate in enumerate(paragraph):\r\n",
        "            for sentence in sentences:\r\n",
        "                if sentence[:4] in candidate:\r\n",
        "                    vec[idx] += 1\r\n",
        "        return vec\r\n",
        "\r\n",
        "\r\n",
        "class Highlight(Text):\r\n",
        "    def __init__(self, text):\r\n",
        "        super().__init__(text)\r\n",
        "        print(\"Highlight\")\r\n",
        "        wordrank_extractor = KRWordRank(min_count=3, max_length=10)\r\n",
        "        self.keywords, rank, graph = wordrank_extractor.extract(self.paragraphs)\r\n",
        "        self.path = \"/content/drive/Shareddrives/KPMG_Ideation/OpenInformationExtraction/frameBERT\"\r\n",
        "        p = []\r\n",
        "        kw = []\r\n",
        "        for k, v in self.keywords.items():\r\n",
        "            p.append(okt.pos(k))\r\n",
        "            kw.append(k)\r\n",
        "        words = self.text.split(' ')\r\n",
        "        s = set()\r\n",
        "        keylist = [word for i in kw for word in words if i in word]\r\n",
        "        keylist = [i for i in keylist if len(i)>2]\r\n",
        "        for i in keylist:\r\n",
        "            if len(i)>2:\r\n",
        "              s.add(i)\r\n",
        "        # print(\"KEYLIST: \",keylist)\r\n",
        "\r\n",
        "        p = [okt.pos(word) for word in s]\r\n",
        "        self.s = set()\r\n",
        "        for idx in range(len(p)):\r\n",
        "            ls = p[idx]\r\n",
        "            for tags in ls:\r\n",
        "              word,tag = tags\r\n",
        "              if tag == \"Noun\":\r\n",
        "                if len(word)>=2:\r\n",
        "                  self.s.add(word)\r\n",
        "        self.keys = []\r\n",
        "        for temp in self.s:\r\n",
        "            self.keys.append(\" \" + str(temp))\r\n",
        "        print(\"KEYWORDS: \", self.keys)\r\n",
        "\r\n",
        "    # def add_tags_conj(self, txt):\r\n",
        "    #     conj = '그리고, 그런데, 그러나, 그래도, 그래서, 또는, 및, 즉, 게다가, 따라서, 때문에, 아니면, 왜냐하면, 단, 오히려, 비록, 예를 들어, 반면에, 하지만, 그렇다면, 바로, 이에 대해'\r\n",
        "    #     conj = conj.replace(\"'\", \"\")\r\n",
        "    #     self.candidates = conj.split(\",\")\r\n",
        "    #     self.newtext = deepcopy(txt)\r\n",
        "    #     self.idx = [(i, i + len(candidate)) for candidate in self.candidates for i in\r\n",
        "    #                     self.findall(candidate, txt)]\r\n",
        "    #     for i in range(len(self.idx)):\r\n",
        "    #         try:\r\n",
        "    #             self.idx = [(start, start + len(candidate)) for candidate in self.candidates for start in\r\n",
        "    #                         self.findall(candidate, self.newtext)]\r\n",
        "    #             word = self.newtext[self.idx[i][0]:self.idx[i][1]]\r\n",
        "    #             self.newtext = word.join([self.newtext[:self.idx[i][0]], self.newtext[self.idx[i][1]:]])\r\n",
        "    #         except:\r\n",
        "    #             pass\r\n",
        "    #     return self.newtext"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaFLquZkZ8Dw"
      },
      "source": [
        "class Summarize(Highlight):\r\n",
        "    def __init__(self, text, paragraph_no):\r\n",
        "      super().__init__(text)\r\n",
        "      print(\"length of paragraphs \",len(self.paragraphs))\r\n",
        "      self.txt = self.paragraphs[paragraph_no]\r\n",
        "      self.paragraph_no = paragraph_no\r\n",
        "\r\n",
        "    def summarize(self):\r\n",
        "        url = \"https://api.smrzr.io/v1/summarize?num_sentences=5&algorithm=kmeans\"\r\n",
        "        headers = {\r\n",
        "            'content-type': 'raw/text',\r\n",
        "            'origin': 'https://smrzr.io',\r\n",
        "            'referer': 'https://smrzr.io/',\r\n",
        "            'sec-fetch-dest': 'empty',\r\n",
        "            'sec-fetch-mode': 'cors',\r\n",
        "            'sec-fetch-site': 'same-site',\r\n",
        "            \"user-agent\": \"Mozilla/5.0\"\r\n",
        "        }\r\n",
        "        resp = requests.post(url, headers=headers, data= self.txt.encode('utf-8'))\r\n",
        "        assert resp.status_code == 200\r\n",
        "        summary = resp.json()['summary']\r\n",
        "        temp = summary.split('\\n')\r\n",
        "        print(\"BERT: \", temp)\r\n",
        "        return temp\r\n",
        "\r\n",
        "\r\n",
        "    def summarizeTextRank(self):\r\n",
        "        tr = TextRank(sent_tokenize)\r\n",
        "        summary = tr.summarize(self.txt, num_sentences=5).split('\\n')\r\n",
        "        print(\"Textrank: \",summary)\r\n",
        "        return summary\r\n",
        "\r\n",
        "\r\n",
        "    def summarizeLexRank(self):\r\n",
        "        lr = LexRank()\r\n",
        "        lr.summarize(self.txt)\r\n",
        "        summaries = lr.probe()\r\n",
        "        print(\"Lexrank: \",summaries)\r\n",
        "        return summaries\r\n",
        "LexRank().summarize()\r\n",
        "    def ensembleSummarize(self):\r\n",
        "        a = np.array(self.countMatcher(self.summarize(), self.paragraph_no))\r\n",
        "        \r\n",
        "        try:\r\n",
        "          b = np.array(self.countMatcher(self.summarizeLexRank(), self.paragraph_no))\r\n",
        "        except:\r\n",
        "          b = np.zeros_like(a)\r\n",
        "        c = np.array(self.countMatcher(self.summarizeTextRank(),self.paragraph_no))\r\n",
        "        result= a+b+c\r\n",
        "        i, = np.where(result == max(result))\r\n",
        "        txt, index = self.docs[self.paragraph_no][i[0]], i[0]\r\n",
        "        return txt, index"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvhPe7pamSUn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cb7f206-46a5-4198-9b5b-41c53727c7a4"
      },
      "source": [
        "result = chked\r\n",
        "high = Highlight(result)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TEXT\n",
            "Highlight\n",
            "KEYWORDS:  [' 직업', ' 여기', ' 거나', ' 확인', ' 짬뽕', ' 시기', ' 해결', ' 영어', ' 경제활동', ' 각자', ' 인구', ' 투여', ' 변수', ' 엄마', ' 프로토타입', ' 무시', ' 플러스', ' 주시', ' 분야', ' 개발', ' 반복', ' 이중', ' 잠시', ' 다른', ' 공모전', ' 회사', ' 건가', ' 혹시', ' 직접', ' 정해진', ' 사실', ' 감염', ' 기술', ' 파악', ' 순위', ' 언어', ' 이해', ' 템플릿', ' 프로젝트', ' 법률용어', ' 각각', ' 달라', ' 가능', ' 저희', ' 수도권', ' 웨이트', ' 훈련', ' 이상', ' 도도', ' 회로', ' 구글', ' 음성', ' 대신', ' 인식', ' 처리', ' 공급망', ' 설정', ' 머리', ' 코로나', ' 운송', ' 고양이', ' 자동', ' 사망률', ' 질병', ' 기관', ' 방안', ' 한국어', ' 전환', ' 제안', ' 스피츠', ' 일정', ' 문제', ' 바이러스', ' 타이틀', ' 목표', ' 이전', ' 디자인', ' 반면', ' 보시', ' 자체', ' 전화', ' 산정', ' 처음', ' 체제', ' 요약', ' 화로', ' 서울대', ' 진행', ' 접종', ' 자원', ' 서울시', ' 목적', ' 보이스', ' 유동', ' 활동', ' 옛날', ' 메인', ' 교체', ' 기반', ' 주제', ' 감염병', ' 승현', ' 서울', ' 위로', ' 언제', ' 지역', ' 형님', ' 가지', ' 이민호', ' 방식', ' 문서', ' 특성', ' 학습', ' 이면', ' 얼마나', ' 조금', ' 의견', ' 얘기', ' 프로세스', ' 근거', ' 목요일', ' 면역', ' 말씀', ' 유리', ' 공급', ' 시작', ' 노출', ' 순서대로', ' 약간', ' 최적화', ' 순서', ' 따로따로', ' 작성', ' 시간', ' 지식', ' 활용', ' 선택', ' 자연어', ' 적용', ' 고려', ' 변경', ' 준비', ' 캡처', ' 반대', ' 이미지', ' 텍스트', ' 정해', ' 이후', ' 이민석', ' 다음', ' 다중언어', ' 거기', ' 경우', ' 취소', ' 시스템', ' 그림', ' 밀도', ' 하나', ' 임베딩', ' 보이', ' 정리', ' 관련', ' 가요', ' 보고', ' 잘못', ' 원래', ' 설명', ' 우리나라', ' 걱정', ' 지역별', ' 경제', ' 이하', ' 모델', ' 민찬', ' 이용', ' 음성인식', ' 사유리', ' 결과', ' 그다음', ' 정보', ' 논란', ' 분비', ' 처럼', ' 영화', ' 조정', ' 부분', ' 손님', ' 배치', ' 핵심', ' 회의', ' 여기저기', ' 아이디어', ' 로이', ' 교수', ' 하나로', ' 정부', ' 알고리즘', ' 하라', ' 제대로', ' 종사', ' 자동화', ' 의학', ' 계속', ' 게임', ' 논문', ' 단계', ' 사전', ' 데이터', ' 지금', ' 기존', ' 전체', ' 대부분', ' 파일', ' 보통', ' 따라서', ' 문장', ' 모델링', ' 나라', ' 가능성', ' 연령', ' 필요', ' 불구', ' 여러분', ' 타이핑', ' 프로토', ' 치사', ' 오히려', ' 사람', ' 때문', ' 아마존', ' 우리', ' 전복', ' 참고', ' 제품', ' 축하', ' 갑자기', ' 그대로', ' 조정은', ' 연결', ' 활동량', ' 김민찬', ' 대해', ' 구매', ' 집단', ' 자기', ' 프로그램', ' 분배', ' 방법', ' 효과', ' 로만', ' 퍼센트', ' 파워포인트', ' 제출', ' 개발자', ' 나중', ' 별로', ' 시간대', ' 투자', ' 수집', ' 모든', ' 프런트엔드', ' 유지', ' 통해', ' 대표', ' 미팅', ' 차이', ' 어쨌든', ' 일단', ' 수요일', ' 모듈', ' 시중', ' 피치', ' 접목', ' 어디', ' 사용', ' 단어', ' 우선', ' 활성화', ' 서비스', ' 대별', ' 전문용어', ' 야외', ' 발전', ' 전문', ' 슬라이드', ' 만약', ' 증가', ' 기본', ' 그거', ' 주제가', ' 생각', ' 저번', ' 사용자', ' 백신', ' 정도', ' 전개', ' 그것', ' 구현', ' 의미']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCqcx5miWDX3"
      },
      "source": [
        "summarizer = Summarize(chked, 0)\r\n",
        "sum, id = summarizer.ensembleSummarize()\r\n",
        "print(\"summarized \",sum)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "bTkaatLht9lU",
        "outputId": "00ab8687-8461-489e-a22b-22d6c62754e3"
      },
      "source": [
        "sum"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'그렇다면 직업 특성상 재택근무가 아예 불가능한 집중된 섞여 있을 거 같은데 직업 특성뿐만이 아니라 그냥 바이러스 노출도 제일 중요한 거 같기도 하고 그 그게 이제 목적에 따라서 누굴 먼저 맞춰야 되니까 조금 다른게 그 감염률을 나 죽고 싶으면 네 사실 뭐랄까 제일 활발하게 활동하는 새끼들을 먼저 맞춰야지 그래야지 지금 뭐 그 퍼트리고 다니는 애들이 안 걸려 있으니까 그래서 그런 경우 조금 야외활동이라던가 이런 것들이 활동이 많은 젊은 층들이 맞춰야 되는 반면에 치사율을 낮추고자를 목표로 하면은 우선은 노인 인구가 가장 큰 높으니까 애들 먼저 맞춰야 된다 이런 일이 있어가지고 달달 말씀이시죠 목적에 따라 달라진다.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WISzsUJKzrrc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qv744gokBRfi"
      },
      "source": [
        "- 사용자 인식(speaker identification)이 됐으면 좋겠다 -- clova NOTE 사용시 해결\r\n",
        ">무료 api는 supervised만 있는 듯\r\n",
        "\r\n",
        "- 시간단위로 잘리는 것 루프 만들기\r\n",
        "\r\n",
        "- 기본 웹프레임워크 만들기\r\n",
        "\r\n",
        "- 아웃풋 어떤 모양일지?\r\n"
      ]
    }
  ]
}